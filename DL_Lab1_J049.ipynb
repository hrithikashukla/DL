{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Lab1_J049.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hrithikashukla/DL/blob/master/DL_Lab1_J049.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LPdeJnLbhP5",
        "colab_type": "text"
      },
      "source": [
        "# Keras package in Python\n",
        "\n",
        "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
        "\n",
        "Use Keras if you need a deep learning library that:\n",
        "\n",
        "Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
        "Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
        "Runs seamlessly on CPU and GPU.\n",
        "Read the documentation at Keras.io.\n",
        "\n",
        "Keras is compatible with: Python 2.7-3.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5qa9OtRbyqO",
        "colab_type": "text"
      },
      "source": [
        "The core data structure of Keras is a **model**, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers. For more complex architectures, you should use the Keras functional API, which allows to build arbitrary graphs of layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6zlphu1bZIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here is the Sequential model:\n",
        "\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHysZQcdcAza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Stacking layers is as easy as .add():\n",
        "\n",
        "from keras.layers import Dense\n",
        "\n",
        "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
        "model.add(Dense(units=1, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBcPLRoqd9iH",
        "colab_type": "text"
      },
      "source": [
        "# Compilation\n",
        "\n",
        "Before training a model, you need to configure the learning process, which is done via the compile method. It receives three arguments:\n",
        "\n",
        "An optimizer. This could be the string identifier of an existing optimizer (such as rmsprop or adagrad), or an instance of the Optimizer class. \n",
        "\n",
        "\n",
        "A loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse), or it can be an objective function. \n",
        "\n",
        "A list of metrics. For any classification problem you will want to set this to metrics=['accuracy']. A metric could be the string identifier of an existing metric or a custom metric function. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD6V0BrQcCNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Once your model looks good, configure its learning process with .compile():\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpokwhIudqU7",
        "colab_type": "text"
      },
      "source": [
        "# Specifying the input shape\n",
        "\n",
        "The model needs to know what input shape it should expect. For this reason, the first layer in a Sequential model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. There are several possible ways to do this:\n",
        "\n",
        "Pass an input_shape argument to the first layer. This is a shape tuple (a tuple of integers or None entries, where None indicates that any positive integer may be expected). In input_shape, the batch dimension is not included.\n",
        "\n",
        "Some 2D layers, such as Dense, support the specification of their input shape via the argument input_dim, and some 3D temporal layers support the arguments input_dim and input_length.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q7KHVXVd47_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate dummy data\n",
        "import numpy as np\n",
        "data = np.random.random((1000, 100))\n",
        "labels = np.random.randint(2, size=(1000, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T2tgwU0e-fX",
        "colab_type": "code",
        "outputId": "fb69973f-de88-4c62-edd3-366ed8202615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Check the shape of inputs\n",
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7wfi1JJfFtu",
        "colab_type": "code",
        "outputId": "b56ba83c-ff55-409e-9d8d-ed5d386f6939",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Check the shape of labels\n",
        "labels.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqYljCVFeLBj",
        "colab_type": "text"
      },
      "source": [
        "# Training\n",
        "\n",
        "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the fit function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG96MZWdeP5F",
        "colab_type": "code",
        "outputId": "3ab1bf26-f6ce-4929-86a9-c8c2c5d68df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "# Train the model, iterating on the data in batches of 32 samples\n",
        "model.fit(data, labels, epochs=10, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 0s 261us/step - loss: 8.0509 - acc: 0.4950\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 0s 42us/step - loss: 8.0509 - acc: 0.4950\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 0s 43us/step - loss: 8.0509 - acc: 0.4950\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 0s 42us/step - loss: 8.0509 - acc: 0.4950\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 0s 50us/step - loss: 8.0509 - acc: 0.4950\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 0s 48us/step - loss: 8.0509 - acc: 0.4950\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 0s 41us/step - loss: 8.0509 - acc: 0.4950\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 0s 44us/step - loss: 8.0509 - acc: 0.4950\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 0s 44us/step - loss: 8.0509 - acc: 0.4950\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 0s 40us/step - loss: 8.0509 - acc: 0.4950\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8f44a635f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUSaXF22foxp",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 1: Build a Sequential model with two layers. \n",
        "\n",
        "Layer 1 is dense with 32 nodes and 'relu' activation function. Input dimension is 100.\n",
        "\n",
        "Layer 2 is dense with 10 nodes and 'softmax' activation function.\n",
        "\n",
        "Compile the model with optimizer 'rmsprop',             loss='categorical_crossentropy' and metrics=['accuracy']\n",
        "\n",
        "\n",
        "Generate dummy data using np.random.random 1000, 100 and dummy labels of 10 categories with size 1000,1\n",
        "\n",
        "\n",
        "## Convert labels to categorical one-hot encoding using below statement\n",
        "from keras.utils import to_categorical\n",
        "one_hot_labels = to_categorical(labels, num_classes=10)\n",
        "\n",
        "Finally, Train the model, iterating on the data in batches of 32 samples\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qcDgrrebvlSO"
      },
      "source": [
        "Double-click <b>here</b> for the solution.\n",
        "\n",
        "<!-- The answer is below:\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "model = Sequential()\n",
        "model.add(Dense(32, activation='relu', input_dim=100))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Generate dummy data\n",
        "import numpy as np\n",
        "data = np.random.random((1000, 100))\n",
        "labels = np.random.randint(10, size=(1000, 1))\n",
        "\n",
        "# Convert labels to categorical one-hot encoding\n",
        "one_hot_labels = to_categorical(labels, num_classes=10)\n",
        "\n",
        "# Train the model, iterating on the data in batches of 32 samples\n",
        "model.fit(data, one_hot_labels, epochs=10, batch_size=32)\n",
        "\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFAuPHHFwOoX",
        "colab_type": "code",
        "outputId": "70ba0359-1f95-49a2-8e63-87589a8334a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "model = Sequential()\n",
        "model.add(Dense(32, activation='relu', input_dim=100))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Generate dummy data\n",
        "import numpy as np\n",
        "data = np.random.random((1000, 100))\n",
        "labels = np.random.randint(10, size=(1000, 1))\n",
        "\n",
        "# Convert labels to categorical one-hot encoding\n",
        "one_hot_labels = to_categorical(labels, num_classes=10)\n",
        "\n",
        "# Train the model, iterating on the data in batches of 32 samples\n",
        "model.fit(data, one_hot_labels, epochs=10, batch_size=32)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 0s 256us/step - loss: 2.3453 - acc: 0.1110\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 0s 41us/step - loss: 2.3154 - acc: 0.1100\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 0s 47us/step - loss: 2.3003 - acc: 0.1130\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 0s 42us/step - loss: 2.2892 - acc: 0.1280\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 0s 40us/step - loss: 2.2780 - acc: 0.1290\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 0s 43us/step - loss: 2.2704 - acc: 0.1490\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 0s 41us/step - loss: 2.2563 - acc: 0.1550\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 0s 42us/step - loss: 2.2496 - acc: 0.1500\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 0s 50us/step - loss: 2.2385 - acc: 0.1750\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 0s 41us/step - loss: 2.2253 - acc: 0.1770\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8f37c45f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVpe2f24jTIW",
        "colab_type": "text"
      },
      "source": [
        "# Exercise 2: Building a Basic Keras Neural Network Sequential Model to classify MNIST dataset\n",
        "\n",
        "First we import package and a set hyperparameter and identify dataset variables.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl3kYpMSjqUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from IPython.display import SVG\n",
        "\n",
        "NUM_ROWS = 28\n",
        "NUM_COLS = 28\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr-2yrEUjt5Q",
        "colab_type": "text"
      },
      "source": [
        "## Next is a function for outputting some simple (but useful) metadata of our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koiwyv_Jj0Yf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_summary(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Summarize current state of dataset\"\"\"\n",
        "    print('Train images shape:', X_train.shape)\n",
        "    print('Train labels shape:', y_train.shape)\n",
        "    print('Test images shape:', X_test.shape)\n",
        "    print('Test labels shape:', y_test.shape)\n",
        "    print('Train labels:', y_train)\n",
        "    print('Test labels:', y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcHaDkegj-LU",
        "colab_type": "text"
      },
      "source": [
        "## Next we load our dataset (MNIST, using Keras' dataset utilities), and then use the function above to get some dataset metadata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeMLUCHHkBJy",
        "colab_type": "code",
        "outputId": "13bec370-f751-44d5-acb2-4d355e5c8466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "# Load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Check state of dataset\n",
        "data_summary(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train images shape: (60000, 28, 28)\n",
            "Train labels shape: (60000,)\n",
            "Test images shape: (10000, 28, 28)\n",
            "Test labels shape: (10000,)\n",
            "Train labels: [5 0 4 ... 5 6 8]\n",
            "Test labels: [7 2 1 ... 4 5 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQBMRuA5kKTQ",
        "colab_type": "text"
      },
      "source": [
        "To feed MNIST instances into a neural network, they need to be reshaped, from a 2 dimensional image representation to a single dimension sequence. We also convert our class vector to a binary matrix (using to_categorical). This is accomplished below, after which the same function defined above is called again in order to show the effects of our data reshaping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRwDGnB8kLpm",
        "colab_type": "code",
        "outputId": "e223161a-91a8-4c34-d73d-920207875891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "# Reshape data\n",
        "X_train = X_train.reshape((X_train.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.reshape((X_test.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_test = X_test.astype('float32') / 255\n",
        "\n",
        "# Categorically encode labels\n",
        "y_train = to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "# Check state of dataset\n",
        "data_summary(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train images shape: (60000, 784)\n",
            "Train labels shape: (60000, 10)\n",
            "Test images shape: (10000, 784)\n",
            "Test labels shape: (10000, 10)\n",
            "Train labels: [[0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]]\n",
            "Test labels: [[0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ho-Gqe4elgoK"
      },
      "source": [
        "#Build a Sequential model of Neural Network with three dense layers.\n",
        "\n",
        "\n",
        "\n",
        "1.   Layer 1, 2 and 3 have 512, 256 and 10 nodes respectively. Activation function of first two layers is relu and final layer is softmax.\n",
        "2.   Compile model with optimizer='rmsprop',               loss='categorical_crossentropy' and metrics=['accuracy']\n",
        "3. Train the model with two additional parameters verbose=1 and validation_data=(X_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2ajV6BzmA8E",
        "colab_type": "text"
      },
      "source": [
        "Double-click <b>here</b> for the solution.\n",
        "\n",
        "<!-- The answer is below:\n",
        "# Build neural network\n",
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2qJdnXF0X8V",
        "colab_type": "code",
        "outputId": "298de43f-e01e-4767-f701-8f9171a44321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(NUM_ROWS * NUM_COLS,)))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.2253 - acc: 0.9308 - val_loss: 0.1132 - val_acc: 0.9635\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 7s 114us/step - loss: 0.0853 - acc: 0.9740 - val_loss: 0.0729 - val_acc: 0.9772\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 7s 114us/step - loss: 0.0550 - acc: 0.9833 - val_loss: 0.0715 - val_acc: 0.9791\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 7s 114us/step - loss: 0.0406 - acc: 0.9873 - val_loss: 0.0719 - val_acc: 0.9810\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 7s 111us/step - loss: 0.0292 - acc: 0.9911 - val_loss: 0.0815 - val_acc: 0.9785\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0232 - acc: 0.9928 - val_loss: 0.0729 - val_acc: 0.9816\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 7s 112us/step - loss: 0.0182 - acc: 0.9942 - val_loss: 0.0816 - val_acc: 0.9822\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 7s 113us/step - loss: 0.0146 - acc: 0.9954 - val_loss: 0.0979 - val_acc: 0.9793\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 7s 115us/step - loss: 0.0138 - acc: 0.9960 - val_loss: 0.0880 - val_acc: 0.9826\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 7s 114us/step - loss: 0.0105 - acc: 0.9966 - val_loss: 0.0957 - val_acc: 0.9811\n",
            "Test loss: 0.09571090894656323\n",
            "Test accuracy: 0.9811\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R1P-fE9mlkV",
        "colab_type": "text"
      },
      "source": [
        "#Output a summary of the neural network we built."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF62Moggmnye",
        "colab_type": "code",
        "outputId": "20852902-9ba2-48f1-96d8-8f0d01b4c950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "# Summary of neural network\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 535,818\n",
            "Trainable params: 535,818\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3r9ODkFmqwX",
        "colab_type": "text"
      },
      "source": [
        "# Visualize the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF5PEZWPmtvf",
        "colab_type": "code",
        "outputId": "bd1aa6a9-ccba-4b3d-fa3c-1b2bf59bb283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "# Output network visualization\n",
        "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"352pt\" viewBox=\"0.00 0.00 188.00 264.00\" width=\"251pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1.3333 1.3333) rotate(0) translate(4 260)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-260 184,-260 184,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140253091951280 -->\n<g class=\"node\" id=\"node1\">\n<title>140253091951280</title>\n<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 180,-255.5 180,-219.5 0,-219.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"90\" y=\"-233.8\">dense_10_input: InputLayer</text>\n</g>\n<!-- 140253091950832 -->\n<g class=\"node\" id=\"node2\">\n<title>140253091950832</title>\n<polygon fill=\"none\" points=\"33,-146.5 33,-182.5 147,-182.5 147,-146.5 33,-146.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"90\" y=\"-160.8\">dense_10: Dense</text>\n</g>\n<!-- 140253091951280&#45;&gt;140253091950832 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140253091951280-&gt;140253091950832</title>\n<path d=\"M90,-219.4551C90,-211.3828 90,-201.6764 90,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"93.5001,-192.5903 90,-182.5904 86.5001,-192.5904 93.5001,-192.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140253093060448 -->\n<g class=\"node\" id=\"node3\">\n<title>140253093060448</title>\n<polygon fill=\"none\" points=\"33.5,-73.5 33.5,-109.5 146.5,-109.5 146.5,-73.5 33.5,-73.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"90\" y=\"-87.8\">dense_11: Dense</text>\n</g>\n<!-- 140253091950832&#45;&gt;140253093060448 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140253091950832-&gt;140253093060448</title>\n<path d=\"M90,-146.4551C90,-138.3828 90,-128.6764 90,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"93.5001,-119.5903 90,-109.5904 86.5001,-119.5904 93.5001,-119.5903\" stroke=\"#000000\"/>\n</g>\n<!-- 140253309274152 -->\n<g class=\"node\" id=\"node4\">\n<title>140253309274152</title>\n<polygon fill=\"none\" points=\"33,-.5 33,-36.5 147,-36.5 147,-.5 33,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"90\" y=\"-14.8\">dense_12: Dense</text>\n</g>\n<!-- 140253093060448&#45;&gt;140253309274152 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140253093060448-&gt;140253309274152</title>\n<path d=\"M90,-73.4551C90,-65.3828 90,-55.6764 90,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"93.5001,-46.5903 90,-36.5904 86.5001,-46.5904 93.5001,-46.5903\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XeWXc0ft6fO",
        "colab_type": "text"
      },
      "source": [
        "#Exercise: Train a Sequential Neural network on CIFAR10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPWjNVe9uCnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import packages\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjXMCDsxuHAc",
        "colab_type": "code",
        "outputId": "f7bd65a5-3c24-4849-8c5f-5f26565bd37a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#import dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "#change shape from image to vector\n",
        "X_train = X_train.reshape(50000, 32 * 32 * 3)\n",
        "X_test = X_test.reshape(10000, 32 * 32 * 3)\n",
        "\n",
        "#preprocess\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255.0\n",
        "X_test /= 255.0\n",
        "\n",
        "#change labels from numeric to one hot encoded\n",
        "Y_train = to_categorical(y_train, 10)\n",
        "Y_test =  to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SV6PPCHu6iy",
        "colab_type": "text"
      },
      "source": [
        "# Build sequential model \n",
        "\n",
        "\n",
        "\n",
        "1.   Sequential model with four layers having 1024,512,512,10 nodes respectively\n",
        "2.   Input shape = 3072\n",
        "3.   Activation of first three layers is relu and final is softmax\n",
        "4.   Compile model using loss='categorical_crossentropy', optimizer='adam' and metrics=['accuracy']\n",
        "5. Train model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t51-PrDOvK0R",
        "colab_type": "text"
      },
      "source": [
        "Double-click <b>here</b> for the solution.\n",
        "\n",
        "<!--\n",
        "model = Sequential()\n",
        "model.add(Dense(1024, input_shape=(3072, )))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        " \n",
        "\n",
        "# training\n",
        "history = model.fit(X_train, Y_train,\n",
        "                        batch_size=128,\n",
        "                        nb_epoch=10,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_test, Y_test))\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxAE5ryN6cAz",
        "colab_type": "code",
        "outputId": "128e2bbf-4569-4aae-f867-768028b4afc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(1024, input_shape=(3072, )))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        " \n",
        "\n",
        "# training\n",
        "history = model.fit(X_train, Y_train,\n",
        "                        batch_size=128,\n",
        "                        nb_epoch=10,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_test, Y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 39s 772us/step - loss: 1.8793 - acc: 0.3212 - val_loss: 1.7007 - val_acc: 0.3913\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 38s 766us/step - loss: 1.6558 - acc: 0.4067 - val_loss: 1.5788 - val_acc: 0.4394\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 39s 782us/step - loss: 1.5598 - acc: 0.4409 - val_loss: 1.5828 - val_acc: 0.4341\n",
            "Epoch 4/10\n",
            "50000/50000 [==============================] - 38s 764us/step - loss: 1.5014 - acc: 0.4622 - val_loss: 1.4981 - val_acc: 0.4658\n",
            "Epoch 5/10\n",
            "50000/50000 [==============================] - 38s 766us/step - loss: 1.4608 - acc: 0.4796 - val_loss: 1.4535 - val_acc: 0.4803\n",
            "Epoch 6/10\n",
            "50000/50000 [==============================] - 37s 748us/step - loss: 1.4178 - acc: 0.4925 - val_loss: 1.4365 - val_acc: 0.4878\n",
            "Epoch 7/10\n",
            "50000/50000 [==============================] - 37s 742us/step - loss: 1.3792 - acc: 0.5059 - val_loss: 1.4433 - val_acc: 0.4849\n",
            "Epoch 8/10\n",
            "50000/50000 [==============================] - 37s 736us/step - loss: 1.3436 - acc: 0.5183 - val_loss: 1.4098 - val_acc: 0.5016\n",
            "Epoch 9/10\n",
            "50000/50000 [==============================] - 37s 732us/step - loss: 1.3101 - acc: 0.5307 - val_loss: 1.4025 - val_acc: 0.4964\n",
            "Epoch 10/10\n",
            "50000/50000 [==============================] - 38s 766us/step - loss: 1.2695 - acc: 0.5439 - val_loss: 1.4129 - val_acc: 0.5053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c81PL1bVxZRZ",
        "colab_type": "text"
      },
      "source": [
        "# Plot accuracy and loss after model is trained. \n",
        "\n",
        "(Caution: Will not work unless previous step is performed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptYUQKCWwTUf",
        "colab_type": "code",
        "outputId": "dd3f5926-0625-43fd-b3b7-b99d0bc58c55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngiHmpYOh/3UAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1JOX52A01Dc",
        "colab_type": "text"
      },
      "source": [
        "# Improving accuracy on Iris dataset\n",
        "\n",
        "\n",
        "\n",
        "1.   Find the accuracy of the below model\n",
        "2.   Improve accuracy by increasing layers, epochs, optimizer or loss metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN6DlpbYytgU",
        "colab_type": "code",
        "outputId": "8a852997-be5f-4443-a6c6-c7af2f983150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "iris_data = load_iris()\n",
        "\t\n",
        "x = iris_data.data\n",
        "y_ = iris_data.target.reshape(-1, 1) # Convert data to a single column\n",
        "\n",
        "# One Hot encode the class labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y = encoder.fit_transform(y_)\n",
        "\n",
        "\n",
        "# Split the data for training and testing\n",
        "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.01)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(10, input_shape=(4,), activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_x, train_y, verbose=2, batch_size=5, epochs=200)\n",
        "\n",
        "# Test on unseen data\n",
        "\n",
        "results = model.evaluate(test_x, test_y)\n",
        "\n",
        "print('Final test set loss: {:4f}'.format(results[0]))\n",
        "print('Final test set accuracy: {:4f}'.format(results[1]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            " - 0s - loss: 1.3929 - acc: 0.0000e+00\n",
            "Epoch 2/200\n",
            " - 0s - loss: 1.2004 - acc: 0.1824\n",
            "Epoch 3/200\n",
            " - 0s - loss: 1.1116 - acc: 0.4932\n",
            "Epoch 4/200\n",
            " - 0s - loss: 1.0565 - acc: 0.6351\n",
            "Epoch 5/200\n",
            " - 0s - loss: 1.0168 - acc: 0.6554\n",
            "Epoch 6/200\n",
            " - 0s - loss: 0.9734 - acc: 0.6689\n",
            "Epoch 7/200\n",
            " - 0s - loss: 0.9232 - acc: 0.6622\n",
            "Epoch 8/200\n",
            " - 0s - loss: 0.8733 - acc: 0.6351\n",
            "Epoch 9/200\n",
            " - 0s - loss: 0.8180 - acc: 0.6689\n",
            "Epoch 10/200\n",
            " - 0s - loss: 0.7640 - acc: 0.6959\n",
            "Epoch 11/200\n",
            " - 0s - loss: 0.7129 - acc: 0.6554\n",
            "Epoch 12/200\n",
            " - 0s - loss: 0.6709 - acc: 0.6554\n",
            "Epoch 13/200\n",
            " - 0s - loss: 0.6232 - acc: 0.7838\n",
            "Epoch 14/200\n",
            " - 0s - loss: 0.5847 - acc: 0.8716\n",
            "Epoch 15/200\n",
            " - 0s - loss: 0.5518 - acc: 0.8649\n",
            "Epoch 16/200\n",
            " - 0s - loss: 0.5235 - acc: 0.8784\n",
            "Epoch 17/200\n",
            " - 0s - loss: 0.4952 - acc: 0.9189\n",
            "Epoch 18/200\n",
            " - 0s - loss: 0.4692 - acc: 0.9595\n",
            "Epoch 19/200\n",
            " - 0s - loss: 0.4500 - acc: 0.9257\n",
            "Epoch 20/200\n",
            " - 0s - loss: 0.4262 - acc: 0.9595\n",
            "Epoch 21/200\n",
            " - 0s - loss: 0.4059 - acc: 0.9459\n",
            "Epoch 22/200\n",
            " - 0s - loss: 0.3853 - acc: 0.9527\n",
            "Epoch 23/200\n",
            " - 0s - loss: 0.3660 - acc: 0.9730\n",
            "Epoch 24/200\n",
            " - 0s - loss: 0.3474 - acc: 0.9797\n",
            "Epoch 25/200\n",
            " - 0s - loss: 0.3372 - acc: 0.9527\n",
            "Epoch 26/200\n",
            " - 0s - loss: 0.3151 - acc: 0.9730\n",
            "Epoch 27/200\n",
            " - 0s - loss: 0.3026 - acc: 0.9730\n",
            "Epoch 28/200\n",
            " - 0s - loss: 0.2877 - acc: 0.9730\n",
            "Epoch 29/200\n",
            " - 0s - loss: 0.2720 - acc: 0.9730\n",
            "Epoch 30/200\n",
            " - 0s - loss: 0.2591 - acc: 0.9730\n",
            "Epoch 31/200\n",
            " - 0s - loss: 0.2495 - acc: 0.9730\n",
            "Epoch 32/200\n",
            " - 0s - loss: 0.2360 - acc: 0.9662\n",
            "Epoch 33/200\n",
            " - 0s - loss: 0.2271 - acc: 0.9662\n",
            "Epoch 34/200\n",
            " - 0s - loss: 0.2182 - acc: 0.9730\n",
            "Epoch 35/200\n",
            " - 0s - loss: 0.2085 - acc: 0.9527\n",
            "Epoch 36/200\n",
            " - 0s - loss: 0.1984 - acc: 0.9797\n",
            "Epoch 37/200\n",
            " - 0s - loss: 0.1933 - acc: 0.9797\n",
            "Epoch 38/200\n",
            " - 0s - loss: 0.1859 - acc: 0.9595\n",
            "Epoch 39/200\n",
            " - 0s - loss: 0.1871 - acc: 0.9595\n",
            "Epoch 40/200\n",
            " - 0s - loss: 0.1751 - acc: 0.9797\n",
            "Epoch 41/200\n",
            " - 0s - loss: 0.1677 - acc: 0.9730\n",
            "Epoch 42/200\n",
            " - 0s - loss: 0.1624 - acc: 0.9797\n",
            "Epoch 43/200\n",
            " - 0s - loss: 0.1597 - acc: 0.9662\n",
            "Epoch 44/200\n",
            " - 0s - loss: 0.1573 - acc: 0.9662\n",
            "Epoch 45/200\n",
            " - 0s - loss: 0.1501 - acc: 0.9730\n",
            "Epoch 46/200\n",
            " - 0s - loss: 0.1476 - acc: 0.9595\n",
            "Epoch 47/200\n",
            " - 0s - loss: 0.1390 - acc: 0.9797\n",
            "Epoch 48/200\n",
            " - 0s - loss: 0.1363 - acc: 0.9730\n",
            "Epoch 49/200\n",
            " - 0s - loss: 0.1346 - acc: 0.9662\n",
            "Epoch 50/200\n",
            " - 0s - loss: 0.1317 - acc: 0.9662\n",
            "Epoch 51/200\n",
            " - 0s - loss: 0.1321 - acc: 0.9730\n",
            "Epoch 52/200\n",
            " - 0s - loss: 0.1236 - acc: 0.9797\n",
            "Epoch 53/200\n",
            " - 0s - loss: 0.1215 - acc: 0.9797\n",
            "Epoch 54/200\n",
            " - 0s - loss: 0.1193 - acc: 0.9730\n",
            "Epoch 55/200\n",
            " - 0s - loss: 0.1196 - acc: 0.9595\n",
            "Epoch 56/200\n",
            " - 0s - loss: 0.1168 - acc: 0.9730\n",
            "Epoch 57/200\n",
            " - 0s - loss: 0.1133 - acc: 0.9797\n",
            "Epoch 58/200\n",
            " - 0s - loss: 0.1210 - acc: 0.9595\n",
            "Epoch 59/200\n",
            " - 0s - loss: 0.1076 - acc: 0.9797\n",
            "Epoch 60/200\n",
            " - 0s - loss: 0.1163 - acc: 0.9595\n",
            "Epoch 61/200\n",
            " - 0s - loss: 0.1114 - acc: 0.9730\n",
            "Epoch 62/200\n",
            " - 0s - loss: 0.1082 - acc: 0.9527\n",
            "Epoch 63/200\n",
            " - 0s - loss: 0.1076 - acc: 0.9797\n",
            "Epoch 64/200\n",
            " - 0s - loss: 0.1028 - acc: 0.9662\n",
            "Epoch 65/200\n",
            " - 0s - loss: 0.1032 - acc: 0.9730\n",
            "Epoch 66/200\n",
            " - 0s - loss: 0.1070 - acc: 0.9730\n",
            "Epoch 67/200\n",
            " - 0s - loss: 0.0981 - acc: 0.9797\n",
            "Epoch 68/200\n",
            " - 0s - loss: 0.0999 - acc: 0.9662\n",
            "Epoch 69/200\n",
            " - 0s - loss: 0.0972 - acc: 0.9797\n",
            "Epoch 70/200\n",
            " - 0s - loss: 0.0958 - acc: 0.9730\n",
            "Epoch 71/200\n",
            " - 0s - loss: 0.0949 - acc: 0.9797\n",
            "Epoch 72/200\n",
            " - 0s - loss: 0.0952 - acc: 0.9662\n",
            "Epoch 73/200\n",
            " - 0s - loss: 0.0918 - acc: 0.9797\n",
            "Epoch 74/200\n",
            " - 0s - loss: 0.0905 - acc: 0.9730\n",
            "Epoch 75/200\n",
            " - 0s - loss: 0.0931 - acc: 0.9662\n",
            "Epoch 76/200\n",
            " - 0s - loss: 0.0903 - acc: 0.9662\n",
            "Epoch 77/200\n",
            " - 0s - loss: 0.0890 - acc: 0.9662\n",
            "Epoch 78/200\n",
            " - 0s - loss: 0.0917 - acc: 0.9730\n",
            "Epoch 79/200\n",
            " - 0s - loss: 0.0898 - acc: 0.9730\n",
            "Epoch 80/200\n",
            " - 0s - loss: 0.0985 - acc: 0.9459\n",
            "Epoch 81/200\n",
            " - 0s - loss: 0.0851 - acc: 0.9730\n",
            "Epoch 82/200\n",
            " - 0s - loss: 0.0873 - acc: 0.9730\n",
            "Epoch 83/200\n",
            " - 0s - loss: 0.0864 - acc: 0.9797\n",
            "Epoch 84/200\n",
            " - 0s - loss: 0.0929 - acc: 0.9527\n",
            "Epoch 85/200\n",
            " - 0s - loss: 0.0912 - acc: 0.9662\n",
            "Epoch 86/200\n",
            " - 0s - loss: 0.0872 - acc: 0.9662\n",
            "Epoch 87/200\n",
            " - 0s - loss: 0.0861 - acc: 0.9730\n",
            "Epoch 88/200\n",
            " - 0s - loss: 0.0829 - acc: 0.9730\n",
            "Epoch 89/200\n",
            " - 0s - loss: 0.0845 - acc: 0.9797\n",
            "Epoch 90/200\n",
            " - 0s - loss: 0.0913 - acc: 0.9662\n",
            "Epoch 91/200\n",
            " - 0s - loss: 0.0874 - acc: 0.9595\n",
            "Epoch 92/200\n",
            " - 0s - loss: 0.0804 - acc: 0.9797\n",
            "Epoch 93/200\n",
            " - 0s - loss: 0.0855 - acc: 0.9662\n",
            "Epoch 94/200\n",
            " - 0s - loss: 0.0838 - acc: 0.9730\n",
            "Epoch 95/200\n",
            " - 0s - loss: 0.0789 - acc: 0.9662\n",
            "Epoch 96/200\n",
            " - 0s - loss: 0.0883 - acc: 0.9662\n",
            "Epoch 97/200\n",
            " - 0s - loss: 0.0850 - acc: 0.9730\n",
            "Epoch 98/200\n",
            " - 0s - loss: 0.0843 - acc: 0.9595\n",
            "Epoch 99/200\n",
            " - 0s - loss: 0.0806 - acc: 0.9730\n",
            "Epoch 100/200\n",
            " - 0s - loss: 0.0774 - acc: 0.9730\n",
            "Epoch 101/200\n",
            " - 0s - loss: 0.0786 - acc: 0.9730\n",
            "Epoch 102/200\n",
            " - 0s - loss: 0.0792 - acc: 0.9662\n",
            "Epoch 103/200\n",
            " - 0s - loss: 0.0784 - acc: 0.9730\n",
            "Epoch 104/200\n",
            " - 0s - loss: 0.0761 - acc: 0.9730\n",
            "Epoch 105/200\n",
            " - 0s - loss: 0.0755 - acc: 0.9730\n",
            "Epoch 106/200\n",
            " - 0s - loss: 0.0761 - acc: 0.9662\n",
            "Epoch 107/200\n",
            " - 0s - loss: 0.0784 - acc: 0.9730\n",
            "Epoch 108/200\n",
            " - 0s - loss: 0.0769 - acc: 0.9662\n",
            "Epoch 109/200\n",
            " - 0s - loss: 0.0743 - acc: 0.9662\n",
            "Epoch 110/200\n",
            " - 0s - loss: 0.0786 - acc: 0.9797\n",
            "Epoch 111/200\n",
            " - 0s - loss: 0.0769 - acc: 0.9662\n",
            "Epoch 112/200\n",
            " - 0s - loss: 0.0750 - acc: 0.9662\n",
            "Epoch 113/200\n",
            " - 0s - loss: 0.0734 - acc: 0.9730\n",
            "Epoch 114/200\n",
            " - 0s - loss: 0.0752 - acc: 0.9797\n",
            "Epoch 115/200\n",
            " - 0s - loss: 0.0739 - acc: 0.9797\n",
            "Epoch 116/200\n",
            " - 0s - loss: 0.0722 - acc: 0.9797\n",
            "Epoch 117/200\n",
            " - 0s - loss: 0.0728 - acc: 0.9730\n",
            "Epoch 118/200\n",
            " - 0s - loss: 0.0729 - acc: 0.9730\n",
            "Epoch 119/200\n",
            " - 0s - loss: 0.0716 - acc: 0.9797\n",
            "Epoch 120/200\n",
            " - 0s - loss: 0.0767 - acc: 0.9662\n",
            "Epoch 121/200\n",
            " - 0s - loss: 0.0720 - acc: 0.9797\n",
            "Epoch 122/200\n",
            " - 0s - loss: 0.0773 - acc: 0.9527\n",
            "Epoch 123/200\n",
            " - 0s - loss: 0.0728 - acc: 0.9797\n",
            "Epoch 124/200\n",
            " - 0s - loss: 0.0713 - acc: 0.9797\n",
            "Epoch 125/200\n",
            " - 0s - loss: 0.0716 - acc: 0.9662\n",
            "Epoch 126/200\n",
            " - 0s - loss: 0.0729 - acc: 0.9662\n",
            "Epoch 127/200\n",
            " - 0s - loss: 0.0737 - acc: 0.9662\n",
            "Epoch 128/200\n",
            " - 0s - loss: 0.0806 - acc: 0.9662\n",
            "Epoch 129/200\n",
            " - 0s - loss: 0.0709 - acc: 0.9730\n",
            "Epoch 130/200\n",
            " - 0s - loss: 0.0815 - acc: 0.9730\n",
            "Epoch 131/200\n",
            " - 0s - loss: 0.0756 - acc: 0.9730\n",
            "Epoch 132/200\n",
            " - 0s - loss: 0.0700 - acc: 0.9730\n",
            "Epoch 133/200\n",
            " - 0s - loss: 0.0704 - acc: 0.9730\n",
            "Epoch 134/200\n",
            " - 0s - loss: 0.0701 - acc: 0.9730\n",
            "Epoch 135/200\n",
            " - 0s - loss: 0.0704 - acc: 0.9797\n",
            "Epoch 136/200\n",
            " - 0s - loss: 0.0691 - acc: 0.9662\n",
            "Epoch 137/200\n",
            " - 0s - loss: 0.0696 - acc: 0.9730\n",
            "Epoch 138/200\n",
            " - 0s - loss: 0.0727 - acc: 0.9662\n",
            "Epoch 139/200\n",
            " - 0s - loss: 0.0701 - acc: 0.9662\n",
            "Epoch 140/200\n",
            " - 0s - loss: 0.0692 - acc: 0.9662\n",
            "Epoch 141/200\n",
            " - 0s - loss: 0.0728 - acc: 0.9662\n",
            "Epoch 142/200\n",
            " - 0s - loss: 0.0692 - acc: 0.9730\n",
            "Epoch 143/200\n",
            " - 0s - loss: 0.0725 - acc: 0.9797\n",
            "Epoch 144/200\n",
            " - 0s - loss: 0.0726 - acc: 0.9662\n",
            "Epoch 145/200\n",
            " - 0s - loss: 0.0720 - acc: 0.9527\n",
            "Epoch 146/200\n",
            " - 0s - loss: 0.0672 - acc: 0.9730\n",
            "Epoch 147/200\n",
            " - 0s - loss: 0.0708 - acc: 0.9730\n",
            "Epoch 148/200\n",
            " - 0s - loss: 0.0708 - acc: 0.9662\n",
            "Epoch 149/200\n",
            " - 0s - loss: 0.0697 - acc: 0.9730\n",
            "Epoch 150/200\n",
            " - 0s - loss: 0.0664 - acc: 0.9730\n",
            "Epoch 151/200\n",
            " - 0s - loss: 0.0662 - acc: 0.9730\n",
            "Epoch 152/200\n",
            " - 0s - loss: 0.0684 - acc: 0.9730\n",
            "Epoch 153/200\n",
            " - 0s - loss: 0.0689 - acc: 0.9730\n",
            "Epoch 154/200\n",
            " - 0s - loss: 0.0685 - acc: 0.9662\n",
            "Epoch 155/200\n",
            " - 0s - loss: 0.0683 - acc: 0.9797\n",
            "Epoch 156/200\n",
            " - 0s - loss: 0.0678 - acc: 0.9730\n",
            "Epoch 157/200\n",
            " - 0s - loss: 0.0686 - acc: 0.9730\n",
            "Epoch 158/200\n",
            " - 0s - loss: 0.0681 - acc: 0.9730\n",
            "Epoch 159/200\n",
            " - 0s - loss: 0.0666 - acc: 0.9730\n",
            "Epoch 160/200\n",
            " - 0s - loss: 0.0688 - acc: 0.9730\n",
            "Epoch 161/200\n",
            " - 0s - loss: 0.0685 - acc: 0.9730\n",
            "Epoch 162/200\n",
            " - 0s - loss: 0.0655 - acc: 0.9662\n",
            "Epoch 163/200\n",
            " - 0s - loss: 0.0728 - acc: 0.9662\n",
            "Epoch 164/200\n",
            " - 0s - loss: 0.0716 - acc: 0.9730\n",
            "Epoch 165/200\n",
            " - 0s - loss: 0.0688 - acc: 0.9662\n",
            "Epoch 166/200\n",
            " - 0s - loss: 0.0724 - acc: 0.9662\n",
            "Epoch 167/200\n",
            " - 0s - loss: 0.0647 - acc: 0.9797\n",
            "Epoch 168/200\n",
            " - 0s - loss: 0.0665 - acc: 0.9797\n",
            "Epoch 169/200\n",
            " - 0s - loss: 0.0668 - acc: 0.9730\n",
            "Epoch 170/200\n",
            " - 0s - loss: 0.0645 - acc: 0.9730\n",
            "Epoch 171/200\n",
            " - 0s - loss: 0.0671 - acc: 0.9730\n",
            "Epoch 172/200\n",
            " - 0s - loss: 0.0657 - acc: 0.9730\n",
            "Epoch 173/200\n",
            " - 0s - loss: 0.0650 - acc: 0.9797\n",
            "Epoch 174/200\n",
            " - 0s - loss: 0.0647 - acc: 0.9730\n",
            "Epoch 175/200\n",
            " - 0s - loss: 0.0660 - acc: 0.9730\n",
            "Epoch 176/200\n",
            " - 0s - loss: 0.0654 - acc: 0.9730\n",
            "Epoch 177/200\n",
            " - 0s - loss: 0.0712 - acc: 0.9662\n",
            "Epoch 178/200\n",
            " - 0s - loss: 0.0658 - acc: 0.9730\n",
            "Epoch 179/200\n",
            " - 0s - loss: 0.0645 - acc: 0.9730\n",
            "Epoch 180/200\n",
            " - 0s - loss: 0.0640 - acc: 0.9730\n",
            "Epoch 181/200\n",
            " - 0s - loss: 0.0750 - acc: 0.9662\n",
            "Epoch 182/200\n",
            " - 0s - loss: 0.0712 - acc: 0.9662\n",
            "Epoch 183/200\n",
            " - 0s - loss: 0.0632 - acc: 0.9730\n",
            "Epoch 184/200\n",
            " - 0s - loss: 0.0648 - acc: 0.9730\n",
            "Epoch 185/200\n",
            " - 0s - loss: 0.0674 - acc: 0.9730\n",
            "Epoch 186/200\n",
            " - 0s - loss: 0.0642 - acc: 0.9730\n",
            "Epoch 187/200\n",
            " - 0s - loss: 0.0641 - acc: 0.9730\n",
            "Epoch 188/200\n",
            " - 0s - loss: 0.0624 - acc: 0.9797\n",
            "Epoch 189/200\n",
            " - 0s - loss: 0.0652 - acc: 0.9730\n",
            "Epoch 190/200\n",
            " - 0s - loss: 0.0670 - acc: 0.9730\n",
            "Epoch 191/200\n",
            " - 0s - loss: 0.0645 - acc: 0.9865\n",
            "Epoch 192/200\n",
            " - 0s - loss: 0.0660 - acc: 0.9797\n",
            "Epoch 193/200\n",
            " - 0s - loss: 0.0632 - acc: 0.9730\n",
            "Epoch 194/200\n",
            " - 0s - loss: 0.0628 - acc: 0.9730\n",
            "Epoch 195/200\n",
            " - 0s - loss: 0.0630 - acc: 0.9797\n",
            "Epoch 196/200\n",
            " - 0s - loss: 0.0628 - acc: 0.9797\n",
            "Epoch 197/200\n",
            " - 0s - loss: 0.0628 - acc: 0.9730\n",
            "Epoch 198/200\n",
            " - 0s - loss: 0.0660 - acc: 0.9730\n",
            "Epoch 199/200\n",
            " - 0s - loss: 0.0618 - acc: 0.9730\n",
            "Epoch 200/200\n",
            " - 0s - loss: 0.0634 - acc: 0.9797\n",
            "2/2 [==============================] - 0s 66ms/step\n",
            "Final test set loss: 0.002770\n",
            "Final test set accuracy: 1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rkZZKV4yV9U",
        "colab_type": "text"
      },
      "source": [
        "#Thank you for completing this notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl-88q4Mu-OH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<!--\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1024, input_shape=(3072, )))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # training\n",
        "    history = model.fit(X_train, Y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        nb_epoch=nb_epoch,\n",
        "                        verbose=1,\n",
        "                        validation_data=(X_test, Y_test))\n",
        "\n",
        "-->"
      ]
    }
  ]
}